---
title: Speed up PyTorch ML Models
description: Techniques to speed up PyTorch Neural Network models using `torch.compile()`, TorchDynamo, TorchInductor and CUDAGraphs.
slug: pytorch-optimization
date: 2024-01-06 00:00:00+0000
image: cover.jpg
tags:
    - PyTorch
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---

Whenever you wrap your model under torch.compile , the model goes through the following steps before execution.


TorchDynamo works by interpreting Python bytecode symbolically, converting it into a graph of tensor operations. If it comes across a segment of code that it cannot interpret, it defaults to the regular Python interpreter. This approach ensures that it can handle a wide range of programs while providing significant performance improvements.

AOT Autograd: It reuses Autograd for Ahead-of-Time (AoT) graphs. AOT Autograd is the automatic differentiation engine in PyTorch 2.0. Its function is to produce backward traces in an ahead-of-time fashion, enhancing the efficiency of the differentiation process. AOT Autograd uses PyTorchâ€™s torch_dispatch mechanism to trace through the existing PyTorch autograd engine, capturing the backward pass ahead of time. This enables acceleration of both the forward and backward pass.


TorchInductor is a deep-learning compiler that translates intermediate representations into executable code. It takes the computation graph generated by TorchDynamo and converts it into optimized low-level kernels. For NVIDIA and AMD GPUs, it employs OpenAI Triton as a fundamental component.

TorchDynamo must be paired with a compiler backend that converts the captured graphs into fast machine code. 


design of TorchInductor to be general, we are starting off with two lower level execution targets

Triton - is a new programming language that provides much higher productivity than CUDA, but with the ability to beat the performance of highly optimized libraries like cuDNN with clean and simple code. It is developed by Philippe Tillet at OpenAI, and is seeing enormous adoption and traction across the industry. Triton supports NVIDIA GPUs, and is quickly growing in popularity as a replacement for hand written CUDA kernels

C++/OpenMP 124 is a widely adopted specification for writing parallel kernels. OpenMP provides a work sharing parallel execution model, and enables support for CPUs. C++ is also an interesting target in that it is a highly portable language and could enable export to more exotic edge devices and hardware architectures.







-------------
 template epilogue fusions, tiling, and horizontal/vertical fusions.
