---
title: Speed up PyTorch ML Models
description: Techniques to speed up PyTorch Neural Network models using `torch.compile()`, TorchDynamo, TorchInductor and CUDAGraphs.
slug: pytorch-optimization
date: 2024-01-06 00:00:00+0000
image: cover.jpg
tags:
    - PyTorch
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---

In this article, we explore ways to speed up your PyTorch models with minimal code changes using `torch.compile` which makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels.


## Inner workings of torch.compile()

In this blog, I will walkthrough various techniques & tools to speed up your PyTorch ML models in particular with `torch.compile` on GPUs.

## What happens when you wrap your model with `torch.compile(model)`?

The model goes through the following steps before execution,

TorchDynamo is responsible for JIT compiling arbitrary Python code into FX graphs, which can then be further optimized. TorchDynamo extracts FX graphs by analyzing Python bytecode during runtime and detecting calls to PyTorch operations.

**TorchDynamo** works by interpreting Python bytecode symbolically, converting it into a graph of tensor operations. If it comes across a segment of code that it cannot interpret, it defaults to the regular Python interpreter. This approach ensures that it can handle a wide range of programs while providing significant performance improvements.

**AOT Autograd** It reuses Autograd for Ahead-of-Time (AoT) graphs. AOT Autograd is the automatic differentiation engine in PyTorch 2.0. Its function is to produce backward traces in an ahead-of-time fashion, enhancing the efficiency of the differentiation process. AOT Autograd uses PyTorch’s torch_dispatch mechanism to trace through the existing PyTorch autograd engine, capturing the backward pass ahead of time. This enables acceleration of both the forward and backward pass.

**TorchInductor** is a deep-learning compiler that translates intermediate representations into executable code. It takes the computation graph generated by TorchDynamo and converts it into optimized low-level kernels. For NVIDIA and AMD GPUs, it employs OpenAI Triton as a fundamental component.


TorchDynamo must be paired with a compiler backend that converts the captured graphs into fast machine code. 


**Triton** is a new programming language that provides much higher productivity than CUDA, but with the ability to beat the performance of highly optimized libraries like cuDNN with clean and simple code. It is developed by Philippe Tillet at OpenAI, and is seeing enormous adoption and traction across the industry. Triton supports NVIDIA GPUs, . It is quickly growing in popularity as a replacement for hand written CUDA kernels.

C++/OpenMP is a widely adopted specification for writing parallel kernels. OpenMP provides a work sharing parallel execution model, and enables support for CPUs.


Let’s now demonstrate that using torch.compile can speed up real models. We will compare standard eager mode and torch.compile by evaluating and training a sample CNN model on CIFAR10 dataset.

## Model performance in Eager mode



## Model performance in torch.compile mode








-------------
 template epilogue fusions, tiling, and horizontal/vertical fusions.
