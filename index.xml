<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Venkata Ravuri</title><link>https://venkataravuri.github.io/</link><description>Recent content on Venkata Ravuri</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 21 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://venkataravuri.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>ML Model Compression - A practicle guide to Pruning, Quantization and Distillation</title><link>https://venkataravuri.github.io/p/pytorch-optimization/</link><pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate><guid>https://venkataravuri.github.io/p/pytorch-optimization/</guid><description>&lt;img src="https://venkataravuri.github.io/cover.jpg" alt="Featured image of post ML Model Compression - A practicle guide to Pruning, Quantization and Distillation" />&lt;p>model compression to improve model performance and deployment efficiency,&lt;/p>
&lt;p>compression techniques, which means making models smaller and simpler without losing their effectiveness.&lt;/p>
&lt;p>Model compression techniques offer benefits such as,&lt;/p>
&lt;ol>
&lt;li>Reduce the size of your model, which makes it easier to store, transfer, and deploy.&lt;/li>
&lt;li>Smaller models also require less memory, which makes them perfect for resource-constrained devices.&lt;/li>
&lt;li>Improve inference speed, allowing for faster predictions and real-time applications.&lt;/li>
&lt;li>Compressed models consume less energy, can be deployed to edge devices such as mobiles, glasses, etc.&lt;/li>
&lt;/ol>
&lt;h2 id="pruning-trimming-the-excess">Pruning: Trimming the Excess
&lt;/h2>&lt;p>&lt;a class="link" href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb" target="_blank" rel="noopener"
>https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb&lt;/a>&lt;/p>
&lt;h2 id="quantization-shrinking-the-precision">Quantization: Shrinking the Precision
&lt;/h2>&lt;h2 id="distillation-knowledge-transfer-for-efficiency">Distillation: Knowledge Transfer for Efficiency
&lt;/h2>&lt;p>&lt;a class="link" href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb" target="_blank" rel="noopener"
>https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb&lt;/a>&lt;/p>
&lt;p>Distillation is a process of training a smaller, more compact model to mimic the behavior of a larger, more complex model.&lt;/p>
&lt;p>By transferring knowledge from the larger model, distillation enables the creation of highly efficient models without sacrificing performance. This technique has been particularly effective in scenarios where computational resources are limited, such as deploying models on edge devices, smartphones, tablets, etc. A large language model can be distilled into a smaller model that retains most of the original model’s performance while being more lightweight and faster to execute.&lt;/p></description></item><item><title>Speed up PyTorch ML Models</title><link>https://venkataravuri.github.io/p/pytorch-optimization/</link><pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate><guid>https://venkataravuri.github.io/p/pytorch-optimization/</guid><description>&lt;img src="https://venkataravuri.github.io/p/pytorch-optimization/cover.jpg" alt="Featured image of post Speed up PyTorch ML Models" />&lt;p>In this article, we explore an elegant way to speed up your PyTorch based ML models with minimal code changes using &lt;code>torch.compile&lt;/code> which makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels. &lt;code>torch.compile&lt;/code> significantly enhances model performance by optimizing both the computation graph and the execution of operations on hardware accelerators, leading to faster inference and training times.&lt;/p>
&lt;h2 id="what-happens-when-you-wrap-your-model-with-torchcompilemodel">What happens when you wrap your model with &lt;code>torch.compile(model)&lt;/code>?
&lt;/h2>&lt;p>When torch.compile is invoked in PyTorch, it performs several background steps to optimize model execution by using several components. The torch.compile model goes through the following steps before execution,&lt;/p>
&lt;p>&lt;strong>TorchDynamo&lt;/strong> is responsible for JIT compiling arbitrary Python code into FX graphs (a graph of tensor operations), which can then be further optimized. TorchDynamo extracts FX graphs by analyzing Python bytecode during runtime and detecting calls to PyTorch operations. If it comes across a segment of code that it cannot interpret, it defaults to the regular Python interpreter. This approach ensures that it can handle a wide range of programs while providing significant performance improvements.&lt;/p>
&lt;p>&lt;strong>AOTAutograd&lt;/strong> Automatically generates the backward computation graph from the forward computation graph, ensuring that gradients can be computed efficiently. Its function is to produce backward traces in an ahead-of-time fashion, enhancing the efficiency of the differentiation process. This enables acceleration of both the forward and backward pass.&lt;/p>
&lt;p>&lt;strong>TorchInductor&lt;/strong> The default backend that compiles the computation graph into optimized low-level code suitable for execution on various hardware accelerators. It takes the computation graph generated by TorchDynamo and converts it into optimized low-level kernels. For NVIDIA and AMD GPUs, it employs OpenAI &lt;strong>Triton&lt;/strong>&lt;/p>
&lt;pre>&lt;code>Triton is a new programming language that provides much higher productivity than CUDA, but with the ability to beat the performance of highly optimized libraries like cuDNN with clean and simple code. It is developed by Philippe Tillet at OpenAI, and is seeing enormous adoption and traction across the industry. Triton supports NVIDIA GPUs, . It is quickly growing in popularity as a replacement for hand written CUDA kernels.
C++/OpenMP is a widely adopted specification for writing parallel kernels. OpenMP provides a work sharing parallel execution model, and enables support for CPUs.
&lt;/code>&lt;/pre>
&lt;p>Let’s now demonstrate that using torch.compile can speed up real models. We will compare standard eager mode and torch.compile by evaluating and training a sample CNN model on CIFAR10 dataset.&lt;/p>
&lt;h2 id="model-performance-in-eager-mode">Model performance in Eager mode
&lt;/h2>&lt;h2 id="model-performance-in-torchcompile-mode">Model performance in torch.compile mode
&lt;/h2>&lt;hr>
&lt;p>template epilogue fusions, tiling, and horizontal/vertical fusions.&lt;/p></description></item><item><title>Archives</title><link>https://venkataravuri.github.io/archives/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://venkataravuri.github.io/archives/</guid><description/></item><item><title>Links</title><link>https://venkataravuri.github.io/links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://venkataravuri.github.io/links/</guid><description>&lt;p>To use this feature, add &lt;code>links&lt;/code> section to frontmatter.&lt;/p>
&lt;p>This page&amp;rsquo;s frontmatter:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">links&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">title&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">GitHub&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">description&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">GitHub is the world&amp;#39;s largest software development platform.&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">website&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">https://github.com&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">title&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">TypeScript&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">description&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">website&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">https://www.typescriptlang.org&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ts-logo-128.jpg&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>image&lt;/code> field accepts both local and external images.&lt;/p></description></item><item><title>Search</title><link>https://venkataravuri.github.io/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://venkataravuri.github.io/search/</guid><description/></item></channel></rss>